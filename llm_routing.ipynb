{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIRST APPROACH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "choices = [\n",
    "    \"Related to Fee transparency: When we have question related to calcualtion of printing the dataframe or sum the fees between a time interval \",\n",
    "    \"Realted to Fees gpt: Vanilla question and annswer ,related to Fee schedule or some general questions\",\n",
    "]\n",
    "def get_choice_str(choices):\n",
    "    choices_str = \"\\n\\n\".join(\n",
    "        [f\"{idx+1}. {c}\" for idx, c in enumerate(choices)]\n",
    "    )\n",
    "    return choices_str\n",
    "\n",
    "\n",
    "choices_str = get_choice_str(choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_prompt0 = PromptTemplate(\n",
    "    \"Some choices are given below. It is provided in a numbered list (1 to\"\n",
    "    \" {num_choices}), where each item in the list corresponds to a\"\n",
    "    \" summary.\\n---------------------\\n{context_list}\\n---------------------\\nUsing\"\n",
    "    \" only the choices above and not prior knowledge, return the top choices\"\n",
    "    \" (no more than {max_outputs}, but only select what is needed) that are\"\n",
    "    \" most relevant to the question: '{query_str}'\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_formatted_prompt(query_str):\n",
    "    fmt_prompt = router_prompt0.format(\n",
    "        num_choices=len(choices),\n",
    "        max_outputs=2,\n",
    "        context_list=choices_str,\n",
    "        query_str=query_str,\n",
    "    )\n",
    "    return fmt_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Related to Fee transparency: When we have question related to calculation of printing the dataframe or sum the fees between a time interval\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "query_str = \"sum fees in 2023\"\n",
    "fmt_prompt = get_formatted_prompt(query_str)\n",
    "openai.api_key=os.getenv('OPENAI_KEY')\n",
    "completion = openai.chat.completions.create(\n",
    "  # Use GPT 3.5 as the LLM\n",
    "  model=\"gpt-4\",\n",
    "  # Pre-define conversation messages for the possible roles \n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": fmt_prompt}\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SECOND APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import fields\n",
    "from pydantic import BaseModel,Field\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choice': {'title': 'Choice', 'description': 'An integer strictly 1 or 2,1 denotes related to Fee transparency and 2 denotes related to Fees Gpt', 'type': 'integer'}, 'reason': {'title': 'Reason', 'description': 'An explanation for the selected choice ', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = [\n",
    "    \"Related to Fee transparency: When we have question related to calcualtion of printing the dataframe or sum the fees between a time interval \",\n",
    "    \"Realted to Fees gpt: Vanilla question and annswer ,related to Fee schedule or some general questions\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field\n",
    "from typing import List\n",
    "class Answer(BaseModel):\n",
    "    choice: int= Field( description=\"An integer strictly 1 or 2,1 denotes related to Fee transparency and 2 denotes related to Fees Gpt\")\n",
    "    reason: str =Field( description=\"An explanation for the selected choice \")\n",
    "\n",
    "\n",
    "class Answers(BaseModel):\n",
    "    \"\"\"Represents a list of answers.\"\"\"\n",
    "\n",
    "    answers: List[Answer]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.program.openai import OpenAIPydanticProgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_prompt1 = PromptTemplate(\n",
    "    \"Some choices are given below. It is provided in a numbered list (1 to\"\n",
    "    \" {num_choices}), where each item in the list corresponds to a\"\n",
    "    \" summary.\\n---------------------\\n{context_list}\\n---------------------\\nUsing\"\n",
    "    \" only the choices above and not prior knowledge, return the top choices\"\n",
    "    \" (no more than {max_outputs}, but only select what is needed) that are\"\n",
    "    \" most relevant to the question: '{query_str}'\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function call: Answers with args: {\"answers\":[{\"choice\":2,\"reason\":\"The choice 'Realted to Fees gpt' is selected because it mentions general questions, which could potentially include information about employee programs at JP Morgan.\"}]}\n"
     ]
    }
   ],
   "source": [
    "query_str = \"Tell me about employee program at JP Morgan\"\n",
    "fmt= router_prompt1.partial_format(\n",
    "    num_choices=len(choices),\n",
    "    max_outputs=len(choices),\n",
    ")\n",
    "import openai\n",
    "import os\n",
    "openai.api_key=os.getenv('OPENAI_KEY')\n",
    "program = OpenAIPydanticProgram.from_defaults(\n",
    "    output_cls=Answers,\n",
    "    prompt=fmt,\n",
    "    verbose=True,\n",
    ")\n",
    "output = program(context_list=choices_str, query_str=query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
